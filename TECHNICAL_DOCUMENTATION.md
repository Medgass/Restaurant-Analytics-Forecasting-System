# ğŸ”§ DOCUMENTATION TECHNIQUE - Restaurant Analytics

## ğŸ“‹ Table des MatiÃ¨res
1. [Architecture](#architecture)
2. [Pipeline de DonnÃ©es](#pipeline-de-donnÃ©es)
3. [ModÃ¨les ImplÃ©mentÃ©s](#modÃ¨les-implÃ©mentÃ©s)
4. [Formules & Calculs](#formules--calculs)
5. [Optimisations](#optimisations)
6. [Troubleshooting](#troubleshooting)

---

## Architecture

### **Vue d'ensemble du Pipeline**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         6 CSV FILES (Raw Data)                          â”‚
â”‚  - sales_transactions (121,640 rows)                   â”‚
â”‚  - daily_factors_sales (731 days)                      â”‚
â”‚  - external_factors (weather, events)                  â”‚
â”‚  - stock_inventory (2,928 items)                       â”‚
â”‚  - clients (500 customers)                             â”‚
â”‚  - products (12 items)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DATA LOADING & VALIDATION (Cell 4)                â”‚
â”‚  - Parse timestamps                                    â”‚
â”‚  - Validate completeness                               â”‚
â”‚  - Check for nulls/duplicates                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA PREPARATION (Cells 5-6)                         â”‚
â”‚  - Daily aggregation (731 rows)                        â”‚
â”‚  - Monthly pivot analysis                              â”‚
â”‚  - Feature engineering (weekday, holidays)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXPLORATORY DATA ANALYSIS (Cell 7)                   â”‚
â”‚  - Temporal trends                                     â”‚
â”‚  - Correlation matrix                                  â”‚
â”‚  - Distribution analysis                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TIME SERIES FORECASTING (Cells 8-10)                 â”‚
â”‚  â”œâ”€ Cell 8: Advanced ETS + External Regressors        â”‚
â”‚  â”œâ”€ Cell 9: ETS Baseline (7-day seasonality)          â”‚
â”‚  â””â”€ Cell 10: Random Forest (Feature-based)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BUSINESS ANALYSIS (Cells 11-13)                      â”‚
â”‚  â”œâ”€ Inventory risk & expiration analysis              â”‚
â”‚  â”œâ”€ RFM customer segmentation                          â”‚
â”‚  â”œâ”€ Product bundle analysis                            â”‚
â”‚  â””â”€ Demand forecasting + Reorder recommendations       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VISUALIZATION & REPORTING (Cells 14-26)              â”‚
â”‚  - 170+ PNG visualizations                             â”‚
â”‚  - 10 CSV reports                                      â”‚
â”‚  - Final business recommendations                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Pipeline de DonnÃ©es

### **1. Chargement (Cell 4)**

```python
# Lecture des 6 fichiers CSV
sales = pd.read_csv('restaurant_sales_transactions.csv', 
                    parse_dates=['date', 'timestamp'])
daily = pd.read_csv('restaurant_daily_factors_sales.csv',
                   parse_dates=['date'])
stock = pd.read_csv('restaurant_stock_inventory.csv',
                   parse_dates=['arrival_date', 'expiration_date'])
```

**Validations:**
- Tous les fichiers prÃ©sents
- Pas de NaN critiques
- Index temporels en ordre croissant

### **2. AgrÃ©gation Quotidienne (Cell 5)**

```python
daily_sales = sales.groupby('date').agg({
    'total_amount': 'sum',    # Revenu
    'quantity': 'sum',        # UnitÃ©s vendues
    'transaction_id': 'nunique'  # Nombre transactions
}).reset_index()

# Fusion avec facteurs externes
daily_ts = daily_sales.merge(external[...], on='date', how='left')
```

**Output:** 731 lignes (2 ans de donnÃ©es)

### **3. Analyse Mensuelle (Cell 6)**

```python
monthly_products = sales.groupby(['month', 'product_name']).agg({
    'quantity': 'sum',
    'total_revenue': 'sum'
}).reset_index()

# Top 5 / Bottom 5 par mois
top5 = monthly_products.groupby('month').apply(
    lambda x: x.nlargest(5, 'total_revenue')
)
```

**Output:** 120 lignes (12 mois Ã— 10 produits)

---

## ModÃ¨les ImplÃ©mentÃ©s

### **1. Advanced ETS + External Regressors (Cell 8)**

**Approche Hybride:**
```
PrÃ©vision Finale = ETS_Forecast + Ridge_Adjustment
```

**DÃ©tails:**
- **ETS Component:** 
  - Trend: Additif
  - Seasonal: Additif (pÃ©riode=7 jours)
  - Lissage exponentiel double

- **Ridge Regression Component:**
  - EntrÃ©es: tempÃ©rature, prÃ©cipitation, weekend
  - Cible: rÃ©sidus ETS
  - Alpha: 1.0

**Performance:**
```
RMSE:  47.27 unitÃ©s
MAPE:  15.48%
RÂ²:    -0.264 (nÃ©gatif sur test - ETS limitation sur pÃ©riode volatile)
```

**Utilisation:** Analyse des effets externes sur la demande

### **2. ETS Baseline (Cell 9)**

**Architecture Simple:**
```python
ets_model = ExponentialSmoothing(
    y_train,
    trend='add',
    seasonal='add',
    seasonal_periods=7  # Hebdomadaire
).fit()
forecast = ets_model.forecast(30)  # 30 jours
```

**Performance:**
```
RMSE:  38.30 unitÃ©s âœ…
MAPE:  11.16%
RÂ²:    0.386 (Acceptable)
```

**Utilisation:** Production (meilleur rapport qualitÃ©/stabilitÃ©)

### **3. Random Forest (Cell 10) â­ MEILLEUR**

**Features utilisÃ©es:**
```python
features = [
    'temperature',          # CorrÃ©lation 0.75 avec demande
    'humidity',
    'precipitation',
    'sunshine_hours',       # CorrÃ©lation 0.74
    'is_weekend',          # CorrÃ©lation 0.38
    'event_impact_factor'
]

model = RandomForestRegressor(
    n_estimators=300,
    max_depth=8,
    random_state=42
)
```

**Feature Importance:**
```
tempÃ©rature          : 42% (dominante)
sunshine_hours       : 28%
is_weekend          : 14%
event_impact_factor : 8%
humidity            : 5%
precipitation       : 3%
```

**Performance:**
```
RMSE:  30.25 unitÃ©s ğŸ† MEILLEUR
MAPE:  10.36%
RÂ²:    0.483 (TrÃ¨s bon)
```

**Utilisation:** RecommandÃ© pour planification

---

## Formules & Calculs

### **1. Metrics d'Ã‰valuation**

#### RMSE (Root Mean Squared Error)
```
RMSE = âˆš(1/n Ã— Î£(y_true - y_pred)Â²)

InterprÃ©tation:
- 30.25: Erreur moyenne en unitÃ©s
- Bonne mÃ©trique pour outliers
```

#### MAPE (Mean Absolute Percentage Error)
```
MAPE = (1/n Ã— Î£|y_true - y_pred|/y_true) Ã— 100%

InterprÃ©tation:
- 10.36%: Le modÃ¨le s'Ã©carte en moyenne de 10.36%
- IndÃ©pendant de l'Ã©chelle
```

#### RÂ² (Coefficient de DÃ©termination)
```
RÂ² = 1 - (SS_res / SS_tot)

InterprÃ©tation:
- 0.483: Le modÃ¨le explique 48.3% de la variance
- Varie de -âˆ Ã  1 (1 = parfait)
- NÃ©gatif = pire qu'une prÃ©diction constante
```

### **2. Stock de SÃ©curitÃ© (Cell 13)**

```
Safety_Stock = Z Ã— Ïƒ_d Ã— âˆšL

OÃ¹:
- Z = 1.645 (pour 95% de niveau de service)
- Ïƒ_d = Ã©cart-type de la demande quotidienne
- L = lead time (7 jours par dÃ©faut)

Exemple:
- Ïƒ_d = 45 unitÃ©s
- L = 7 jours
- Safety_Stock = 1.645 Ã— 45 Ã— âˆš7 = 197 unitÃ©s
```

### **3. QuantitÃ© Ã  Commander**

```
Reorder_Qty = max(0, Demand_LeadTime + Safety_Stock - Current_Stock)

Exemple:
- Demand sur lead time: 350 unitÃ©s
- Safety stock: 197 unitÃ©s
- Stock actuel: 250 unitÃ©s
- Reorder_Qty = max(0, 350 + 197 - 250) = 297 unitÃ©s
```

### **4. Score de Risque d'Expiration**

```
Expiry_Risk_Score = 0.7 Ã— Ratio_NearExpiry + 0.3 Ã— Qty_Normalized

OÃ¹:
- Ratio_NearExpiry = Qty_expiring_soon / Total_Qty
- Qty_Normalized = (NearExpiry_Qty - min) / (max - min)

PondÃ©rations:
- 70% = ratio (importance relative)
- 30% = quantitÃ© (volume absolu)
```

### **5. RFM Score**

```
Recency: Jours depuis dernier achat
Frequency: Nombre de transactions
Monetary: Valeur totale dÃ©pensÃ©e

KMeans clustering sur (R, F, M) normalisÃ©s
Silhouette score utilisÃ© pour k optimale
```

---

## Optimisations

### **1. Gestion des DonnÃ©es Volumineuses**

```python
# Au lieu de charger tout en mÃ©moire
sales = pd.read_csv('large_file.csv')  # RisquÃ©

# Utiliser chunks
chunks = pd.read_csv('large_file.csv', chunksize=10000)
df = pd.concat(chunks)  # Ou traiter par chunk
```

### **2. Vectorization avec NumPy**

```python
# Lent (boucle Python)
for i in range(len(data)):
    result[i] = data[i] * 2

# Rapide (vectorisÃ©)
result = data * 2  # NumPy/Pandas
```

### **3. Mise en Cache des ModÃ¨les**

```python
# Les modÃ¨les ETS et RF sont stockÃ©s en mÃ©moire
# Pas de recalcul Ã  chaque cellule
ets_model, rf_model, etc.
```

### **4. RÃ©duction de la DimensionnalitÃ©**

```python
# Utiliser features importantes pour RF
# Ã‰viter curse of dimensionality
# 6 features â†’ optimal pour ensemble methods
```

---

## Troubleshooting

### **âŒ Erreur: Prophet backend**

```
Error: 'Prophet' object has no attribute 'stan_backend'
Root Cause: CmdStan initialization fails in Python 3.13.5
Solution: Utiliser ETS (meilleure performance de toute faÃ§on)
```

### **âŒ Erreur: CSV not found**

```
Error: FileNotFoundError
Solution:
1. VÃ©rifier tous les 6 fichiers CSV prÃ©sents
2. VÃ©rifier chemins relatifs vs absolus
3. Utiliser Path('.').glob('*.csv') pour dÃ©couvrir
```

### **âŒ Erreur: NaN dans prÃ©visions**

```
Error: NaN values in forecast output
Cause: DonnÃ©es manquantes ou sÃ©ries trop courtes
Solution:
1. Remplir NaN: df.fillna(method='ffill')
2. VÃ©rifier min 14 jours d'historique (pour ETS)
3. Fallback vers moyenne simple si data < min
```

### **âŒ Erreur: MÃ©moire insuffisante**

```
Error: MemoryError
Solution:
1. RÃ©duire TOP_N_PRODUCTS (dÃ©faut=50, min=12)
2. Utiliser chunks pour agrÃ©gation
3. Supprimer visualisations intermÃ©diaires
```

### **âŒ Erreur: Kernel crash**

```
Solution:
1. Kernel â†’ Restart & Clear All Output
2. ExÃ©cuter Cell â†’ Run All
3. Si problÃ¨me persiste: redÃ©marrer Jupyter
```

---

## Performance Benchmarks

### **Cellule par Cellule**

| Cellule | OpÃ©ration | Temps | DÃ©pendance |
|---------|-----------|-------|-----------|
| 4 | CSV Load (6 files, 125K rows) | 216ms | - |
| 5 | Daily aggregation | 23ms | Cell 4 |
| 6 | Monthly pivot (120 rows) | 3,972ms | Cell 4 |
| 7 | EDA plots (2 viz) | 914ms | Cell 5 |
| 8 | Advanced forecast | 1,701ms | Cell 5 |
| 9 | ETS baseline | 209ms | Cell 5 |
| 10 | Random Forest (300 trees) | 386ms | Cell 5 |
| 11 | Inventory analysis | 407ms | Cell 4 |
| 12 | Commercial strategy | 8,898ms | Cell 4, 11 |
| 13 | Demand forecast (12 products) | 1,227ms | Cell 5 |
| 14-26 | Visualizations | ~8,500ms | Various |

**Total:** ~24 secondes pour exÃ©cution complÃ¨te

### **Optimization Opportunities**

```python
# Prioriser:
# 1. Cell 12 (8.9s) - pourrait Ãªtre parallÃ©lisÃ©
# 2. Cell 6 (4.0s) - pivot peut Ãªtre optimisÃ©
# 3. Cell 14-26 (8.5s) - plots gÃ©nÃ©rÃ©s sÃ©quentiellement

# AmÃ©lioration possible:
# - Parallel processing (multiprocessing)
# - Caching intermÃ©diaire
# - Lazy evaluation des plots
```

---

## Fichiers de Configuration

### **ParamÃ¨tres Modifiables**

```python
# Cell 8: Advanced Forecast
FORECAST_HORIZON = 30      # Jours Ã  prÃ©voir
USE_EXTERNAL_REGRESSORS = True

# Cell 13: Demand Forecast
TOP_N_PRODUCTS = 50        # Nombre produits (min=12, tous=None)
LEAD_TIME_DAYS = 7        # DÃ©lai fournisseur
SERVICE_LEVEL = 0.95      # ProbabilitÃ© en stock (95% = Z=1.645)
FORECAST_DAYS = 30        # Horizon prÃ©vision

# Cell 12: RFM Clustering
MAX_K = 6                 # Nombre clusters max Ã  tester
THRESHOLD_RATIO = 0.2     # Ratio expiration pour "risque"
```

---

## Version Control

```
Version: 1.0 - Production Ready
Date: 9 DÃ©cembre 2025
Python: 3.13.5
Jupyter: Compatible avec JupyterLab 4.x

Commits clÃ©s:
- Fix Prophet backend incompatibility
- Optimize ETS forecasting
- Add Random Forest model
- Complete commercial strategy analysis
```

---

## RÃ©fÃ©rences & Ressources

### **Time Series Forecasting**
- Holt-Winters ETS: https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html
- Random Forest: https://scikit-learn.org/stable/modules/ensemble.html#random-forests

### **Inventory Management**
- Safety Stock Formula: https://en.wikipedia.org/wiki/Safety_stock
- Service Level: https://en.wikipedia.org/wiki/Service_level

### **Customer Segmentation**
- RFM Analysis: https://en.wikipedia.org/wiki/RFM_%28customer_value%29
- K-Means Clustering: https://scikit-learn.org/stable/modules/clustering.html#k-means

